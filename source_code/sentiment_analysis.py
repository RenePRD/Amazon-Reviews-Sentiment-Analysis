# -*- coding: utf-8 -*-
"""FinalExam.ipynb

Automatically generated by Colab.

Original file is located at
    https://colab.research.google.com/drive/1p0-P41HAa0aLqI2sSLMomqlNJFf7lBsX
"""

import numpy as np
import pandas as pd
import re
import nltk
import spacy
import string

import io
from google.colab import files

# Import the packages and print their versions
import tensorflow as tf
import keras
import scikeras

print(f"TensorFlow version: {tf.__version__}")
print(f"Keras version: {keras.__version__}")
print(f"SciKeras version: {scikeras.__version__}")

#!pip install tensorflow==2.15.0 keras==2.15.0 scikeras==0.11.0

"""# Upload data set"""

# Check if data is already uploaded, if so, no need to run this cell or the next cell
# skip to the cell that loads data into panda data frame
# this cell just loads the file into the folders on the left hand side
uploaded = files.upload()

#upload the test data
uploaded = files.upload()

# upload data into panda dataframes
train_raw_df = pd.read_csv('train.csv')
test_raw_df = pd.read_csv('test.csv')

#lets check if the data was uploaded properply
train_raw_df

test_raw_df.columns

test_raw_df

test_raw_df.columns



"""# Data Preprocessing"""

# turn the Amazon review text column all into lower case
train_raw_df['reviewText'] = train_raw_df['reviewText'].str.lower()
test_raw_df['reviewText'] = test_raw_df['reviewText'].str.lower()

# check top 5 rows of each dataframe to see if they are lower case
train_raw_df.head()

test_raw_df.head()

# let's see what punctuation marks we can remove
string.punctuation

# let's make the python function that we can use to remove punctuation marks from the review text column
def remove_punctuation(text):
    return text.translate(str.maketrans('', '', string.punctuation))

# let's use our python function remove punctuation marks on our reviewText column
train_raw_df['reviewText'] = train_raw_df['reviewText'].apply(remove_punctuation)
test_raw_df['reviewText'] = test_raw_df['reviewText'].apply(remove_punctuation)

# let's check the if the punctuation marks are removed for the train data set
train_raw_df.head()

# let's check the if the punctuation marks are removed for the test data set
test_raw_df.head()

# let's import the nltk python library and import the stopwords module from it and look at the stopwords that we can remove

#Stopwords remval:
import nltk
nltk.download('stopwords')
from nltk.corpus import stopwords

", ".join(stopwords.words('english'))

# put the stopwords into a set
STOPWORDS = set(stopwords.words('english'))

# define a function that will remove the stop words from our text
def remove_stopwords(text):
    return " ".join([word for word in str(text).split() if word not in STOPWORDS])

# let's use our python function remove stopwords on our reviewText column
train_raw_df['reviewText'] = train_raw_df['reviewText'].apply(remove_stopwords)
test_raw_df['reviewText'] = test_raw_df['reviewText'].apply(remove_stopwords)

# let's check the if the stopwords are removed for the train data set
train_raw_df.head()

# let's check the if the stopwords are removed for the test data set
test_raw_df.head()

# download and import lemmatization libraries
import nltk
nltk.download('wordnet')
nltk.download('averaged_perceptron_tagger')

from nltk.corpus import wordnet
from nltk.stem import WordNetLemmatizer

# use lemmatizer and create dictionary for ntlk.pos_tags and WordNet's part of speech tags
lemmatizer = WordNetLemmatizer()
wordnet_map = {"N":wordnet.NOUN, "V":wordnet.VERB, "J":wordnet.ADJ, "R":wordnet.ADV}

# create function for lemmatization
def lemmatize_words(text):
    pos_tagged_text = nltk.pos_tag(text.split())
    return " ".join([lemmatizer.lemmatize(word, wordnet_map.get(pos[0], wordnet.NOUN)) for word, pos in pos_tagged_text])

# apply lemmatization function to the reviewText column of both train and test dataset
train_raw_df['reviewText'] = train_raw_df['reviewText'].apply(lemmatize_words)
test_raw_df['reviewText'] = test_raw_df['reviewText'].apply(lemmatize_words)

# check head of train dataset
train_raw_df.head()

# check head of test dataset
test_raw_df.head()

"""# Feature Extraction

### Bag of Words
"""

# to get TD-IDF scores, we need the TF IDF vectorizer
# https://scikit-learn.org/stable/modules/generated/sklearn.feature_extraction.text.TfidfVectorizer.html
# import
from sklearn.feature_extraction.text import TfidfVectorizer

# create a bag of words matrix for train dataset
vectorizer = TfidfVectorizer(max_features=3000, stop_words='english')
X_train_counts = vectorizer.fit_transform(train_raw_df['reviewText'])

X_train_counts

# load matrix into a dataframe and check it out
X_train_counts_df = pd.DataFrame(X_train_counts.toarray(), columns=vectorizer.get_feature_names_out())
X_train_counts_df.head(5)

# calculate number of rows X_train_counts_df using .shape
X_train_counts_df.shape

# do the same for test dataset and check it out
X_test_counts = vectorizer.transform(test_raw_df['reviewText'])
X_test_counts_df = pd.DataFrame(X_test_counts.toarray(), columns=vectorizer.get_feature_names_out())
X_test_counts_df.head(5)

# calculate number of rows X_test_counts_df using .shape
X_test_counts_df.shape

"""### Word2Vec"""

from google.colab import drive
drive.mount('/content/drive')

# import gensim library for word2vec
import gensim

# get the pretrained word2vec google model
model = gensim.models.KeyedVectors.load_word2vec_format('/content/drive/MyDrive/GoogleNews-vectors-negative300.bin', binary=True)

# get the vocabulary of the documents from the vectorizer in train dataset
vocabulary = vectorizer.get_feature_names_out()
vocabulary

# create an empty list to store the document vectors
document_vectors = []

# lets go through each document in the TF-IDF matrix
for document_index in range(X_train_counts.shape[0]):

    # get the indices from the X_train_counts TF-IDF matrix
    indices = X_train_counts[document_index].indices

    # get the TF-IDF scores for the current document
    tfidf_scores = X_train_counts[document_index].data

    # make numpy array filled with zeroes for the document vector of this current document
    # we use 300 zeroes because the dimension of the google array is 300
    document_vector = np.zeros(300)

    # pair up the index with the corresponding tf-idf score and go through each word
    for index, tfidf_score in zip(indices, tfidf_scores):
      # get the word from the vocabulary
      word = vocabulary[index]

      # check if the word is in the Google word2vec model
      if word in model:
            # get the word vector
            word_vector = model[word]
            # multiply the tf-idf score with the word vector and add it to the document vector
            document_vector += tfidf_score * word_vector

    # append the document vector to the list
    document_vectors.append(document_vector)

# let's look at the first three document vectors
for i in range(3):
    print(f"Document {i+1} vector: {document_vectors[i]}")

# now let's do the same for the test data
# we already have the vocabulary
# lets make an empty list to house the document vectors in the test dataset
document_vectors_test = []

# lets go through each document in the TF-IDF matrix for the test dataset
for document_index in range(X_test_counts.shape[0]):

    # get the indices from the X_test_counts TF-IDF matrix
    indices = X_test_counts[document_index].indices

    # get the TF-IDF scores for the current document
    tfidf_scores = X_test_counts[document_index].data

    # make numpy array filled with zeroes for the document vector of this current document
    document_vector = np.zeros(300)

    # pair up the index with the corresponding tf-idf score and go through each word
    for index, tfidf_score in zip(indices, tfidf_scores):
      # get the word from the vocabulary
      word = vocabulary[index]

      # check if the word is in the Google word2vec model
      if word in model:
            # get the word vector
            word_vector = model[word]
            # multiply the tf-idf score with the word vector and add it to the document vector
            document_vector += tfidf_score * word_vector

    # append the document vector to the list
    document_vectors_test.append(document_vector)

# Let's look at the first three test document vectors for the test data
for i in range(3):
    print(f"Document {i+1} vector: {document_vectors_test[i]}")

"""# Sentiment Analysis

### Boosted decision tree with BoW
"""

# import the boosted tree model
from xgboost import XGBClassifier

# make the boosted tree model with default parameters
model_xgb = XGBClassifier()

# lets make the random search parameters
n_estimators = [50, 100, 150]
max_depth = [3, 4, 5]
learning_rate = [0.01, 0.1, 0.2]
subsample = [0.8, 0.9, 1.0]

# put parameter grid into a dictionary
parameter_grid = dict(
    n_estimators=n_estimators,
    learning_rate=learning_rate,
    max_depth=max_depth,
    subsample=subsample
)

# let's import random search to get the best hyper parameters
from sklearn.model_selection import RandomizedSearchCV

# use the random search because grid search used up all the ram and crashed the connection
random_search = RandomizedSearchCV(estimator=model_xgb, param_distributions=parameter_grid, scoring='accuracy', cv=3, n_jobs=-1, random_state=42, n_iter=5)

# fit the random search object to the training data
random_result = random_search.fit(X_train_counts_df, train_raw_df['Positive'])

"""Optimized hyperparameters for boosted decision tree with BoW"""

# summarize results
print("Best: %f using %s" % (random_result.best_score_, random_result.best_params_))

# let's recreate the model using the best parameter values
best_model_xgb_bow = XGBClassifier(learning_rate=0.1, max_depth=3, n_estimators=150, subsample=0.8)

# Fit the model
best_model_xgb_bow.fit(X_train_counts_df, train_raw_df['Positive'])

# evaluate the keras model on the train set
accuracy_bow = best_model_xgb_bow.score(X_train_counts_df, train_raw_df['Positive'])
print('Accuracy: %.2f' % (accuracy_bow * 100))

"""### Boosted decision tree with Word2Vec"""

# let's prepare/process the document vectors
# currently, the document vectors (for both train and test data set are lists of numpyarrays(with the 300 dimensions))
# let's change them to numpy arrays of numpy arrays of test vectors, to be used in the model
document_vectors_np = np.array(document_vectors)
document_vectors_test_np = np.array(document_vectors_test)

# make the boosted tree model with default parameters
model_xgb_word2vec = XGBClassifier()

# lets create the random search object for this model with similar parameters
random_search_word2vec = RandomizedSearchCV(estimator=model_xgb_word2vec, param_distributions=parameter_grid, scoring='accuracy', cv=3, n_jobs=-1, random_state=42, n_iter=5)

# fit the random search object to the training data
random_result_word2vec = random_search_word2vec.fit(document_vectors_np, train_raw_df['Positive'])

"""Optimized hyperparameters for boosted decision tree with Word2Vec"""

# summarize results
print("Best: %f using %s" % (random_result_word2vec.best_score_, random_result_word2vec.best_params_))

# lets recreate the model using the best parameter values
best_model_xgb_word2vec = XGBClassifier(learning_rate=0.1, max_depth=3, n_estimators=150, subsample=0.8)

# Fit the model
best_model_xgb_word2vec.fit(document_vectors_np, train_raw_df['Positive'])

# Evaluate the accuracy
accuracy_word2vec = best_model_xgb_word2vec.score(document_vectors_np, train_raw_df['Positive'])
print('Accuracy: %.2f' % (accuracy_word2vec * 100))

"""we end up with the same best parameters

### Feed-Forward Neural Network with BoW
"""

# define the feed-forward neural network model using Keras:
# we import Keras first
import tensorflow as tf
from tensorflow.keras.models import Sequential
from tensorflow.keras.layers import Dense, InputLayer

# let's create function that will make the neural network model
# we'll define the function so that the hyperparameters are the layers, number of nodes/neurons and the activation
# in its parameters, we will default with a small values for arguments, for number of layers, nodes and an arbitrarily chosen activation function
def nnet_model_bow(number_of_layers=1, number_of_nodes=8, activation_function='relu'):
    model = Sequential()
    model.add(InputLayer(input_shape=(X_train_counts_df.shape[1],)))  # Assuming BoW features as input
    for _ in range(number_of_layers):
        model.add(Dense(number_of_nodes, activation=activation_function))
    model.add(Dense(1, activation='sigmoid'))  # Assuming binary classification
    model.compile(optimizer='adam', loss='binary_crossentropy', metrics=['accuracy'])
    return model

# import KeraClassifier and GridSearch
import scikeras
from scikeras.wrappers import KerasClassifier

# create KerasClassifer object, which is the model to use for GridSearch
bow_nnet_model = KerasClassifier(build_fn=nnet_model_bow, verbose=0)

#define the hyperparameter grid for the neural network
param_grid = {
    'model__number_of_layers': [2, 3, 4],
    'model__number_of_nodes': [16, 32, 64],
    'model__activation_function': ['relu', 'sigmoid', 'tanh']
}

# create the random search object
random_search_nnet_bow = RandomizedSearchCV(estimator=bow_nnet_model, param_distributions=param_grid, cv=3, n_jobs=-1, random_state=42, n_iter=5)

# fit the random search object to the training data
random_result_nnet_bow = random_search_nnet_bow.fit(X_train_counts_df, train_raw_df['Positive'])

# summarize the results
print("Best: %f using %s" % (random_result_nnet_bow.best_score_, random_result_nnet_bow.best_params_))

# create the model with best parameters
best_model_nnet_bow = nnet_model_bow(number_of_layers=3, number_of_nodes=16, activation_function='tanh')

# Evaluate the model on the training set
_, accuracy_nnet_bow = best_model_nnet_bow.evaluate(X_train_counts_df, train_raw_df['Positive'], verbose=0)
print('Accuracy: %.2f' % (accuracy_nnet_bow * 100))

"""### Feed-Forward Neural Network with Word2Vec"""

# now let's do the same for Word2vec
# We'll have to define a new function because we've inputted teh shap of the bow data in the function
# Define the neural network model function for Word2Vec features
def nnet_model_word2vec(number_of_layers=1, number_of_nodes=8, activation_function='relu'):
    model = Sequential()
    model.add(InputLayer(input_shape=(document_vectors_np.shape[1],)))  # Word2Vec features input shape
    for _ in range(number_of_layers):
        model.add(Dense(number_of_nodes, activation=activation_function))
    model.add(Dense(1, activation='sigmoid'))  # Assuming binary classification
    model.compile(optimizer='adam', loss='binary_crossentropy', metrics=['accuracy'])
    return model

# Create the KerasClassifier object
word2vec_nnet_model = KerasClassifier(model=nnet_model_word2vec, verbose=0)

# we can use the same param grids from bow
# Create the random search object
random_search_nnet_word2vec = RandomizedSearchCV(estimator=word2vec_nnet_model, param_distributions=param_grid, cv=3, n_jobs=-1, random_state=42, n_iter=5)

# Fit the random search object to the Word2Vec document vectors
random_result_nnet_word2vec = random_search_nnet_word2vec.fit(document_vectors_np, train_raw_df['Positive'])

# Summarize the results
print("Best: %f using %s" % (random_result_nnet_word2vec.best_score_, random_result_nnet_word2vec.best_params_))

# create the model with the best parameters
best_model_nnet_word2vec = nnet_model_word2vec(number_of_layers=4, number_of_nodes=64, activation_function='relu')

# evaluate the model on the training set
_, accuracy_nnet_word2vec = best_model_nnet_word2vec.evaluate(document_vectors_np, train_raw_df['Positive'], verbose=0)
print('Accuracy for Neural Network with Word2Vec: %.2f' % (accuracy_nnet_word2vec * 100))

"""# F1-score of the 4 models"""

# lets import f1 score
from sklearn.metrics import f1_score

"""### boosted decision tree with Bow"""

# get predictions for the test set
y_pred_bow = best_model_xgb_bow.predict(X_test_counts_df)

# get F1 scores
f1_bow = f1_score(test_raw_df['Positive'], y_pred_bow)
print("F1 Score for Boosted Decision Tree with BoW:", f1_bow)

"""### boosted decision tree word2vec"""

# same for word2vec boosted tree
# predict with best model
y_pred_word2vec = best_model_xgb_word2vec.predict(document_vectors_test_np)

# F1 score
f1_word2vec = f1_score(test_raw_df['Positive'], y_pred_word2vec)
print("F1 Score for Boosted Decision Tree with Word2Vec:", f1_word2vec)

"""### Feed-forward neural network Bow"""

#predict
y_pred_nnet_bow = random_result_nnet_bow.predict(X_test_counts_df)

# F1 score
f1_nnet_bow = f1_score(test_raw_df['Positive'], y_pred_nnet_bow)
print("F1 Score for Neural Network with BoW:", f1_nnet_bow)

"""### Feed-forward neural network word2vec

"""

# predict
y_pred_nnet_word2vec = random_result_nnet_word2vec.predict(document_vectors_test_np)

# F1 score
f1_nnet_word2vec = f1_score(test_raw_df['Positive'], y_pred_nnet_word2vec)
print("F1 Score for Neural Network with Word2Vec:", f1_nnet_word2vec)

"""#Predict probability distribution histograms 4 models

"""

# import pyplot
import matplotlib.pyplot as plt

"""we use the xgb predict_proba function to get the probabilities for the xgb classifier model, because the predictions lead to either 1 or 0.
This function will gives the probability estimates for both class(0 and 1). We don't need this for the neural network because the sigmoid function outputs into decimals already
https://xgboost.readthedocs.io/en/stable/python/python_api.html

### boosted decision tree with Bow
"""

# predictions test set
proba_bow = best_model_xgb_bow.predict_proba(X_test_counts_df)
test_raw_df['prediction_bow'] = proba_bow[:, 1]  # let's get the positive class probability

# plot the predictions
plt.hist(test_raw_df.loc[test_raw_df['Positive'] == 1, 'prediction_bow'], bins=50, alpha=0.5, label='target true')
plt.hist(test_raw_df.loc[test_raw_df['Positive'] == 0, 'prediction_bow'], bins=50, alpha=0.5, label='target false')
plt.legend(loc='upper right')
plt.show()

"""yaxis = frequency

the x axis the probability of it being positive

### boosted decision tree wiht Word2vec
"""

# we do the same for this
# predictions test set
proba_word2vec = best_model_xgb_word2vec.predict_proba(document_vectors_test_np)
test_raw_df['prediction_word2vec'] = proba_word2vec[:, 1]

# Plot the predictions
plt.hist(test_raw_df.loc[test_raw_df['Positive'] == 1, 'prediction_word2vec'], bins=50, alpha=0.5, label='target true')
plt.hist(test_raw_df.loc[test_raw_df['Positive'] == 0, 'prediction_word2vec'], bins=50, alpha=0.5, label='target false')
plt.legend(loc='upper right')
plt.show()

"""### feed-forward neural network bow

"""

proba_nnet_bow = best_model_nnet_bow.predict(X_test_counts_df)
test_raw_df['prediction_nnet_bow'] = proba_nnet_bow[:, 0] # we use 0 here because theres only the one column of decimal predictions

plt.hist(test_raw_df.loc[test_raw_df['Positive'] == 1, 'prediction_nnet_bow'], bins=50, alpha=0.5, label='target true')
plt.hist(test_raw_df.loc[test_raw_df['Positive'] == 0, 'prediction_nnet_bow'], bins=50, alpha=0.5, label='target false')
plt.legend(loc='upper right')
plt.show()

"""### feed forward neural netowrk word2vec"""

proba_nnet_word2vec = best_model_nnet_word2vec.predict(document_vectors_test_np)
test_raw_df['prediction_nnet_word2vec'] = proba_nnet_word2vec[:, 0]

plt.hist(test_raw_df.loc[test_raw_df['Positive'] == 1, 'prediction_nnet_word2vec'], bins=50, alpha=0.5, label='target true')
plt.hist(test_raw_df.loc[test_raw_df['Positive'] == 0, 'prediction_nnet_word2vec'], bins=50, alpha=0.5, label='target false')
plt.legend(loc='upper right')
plt.show()